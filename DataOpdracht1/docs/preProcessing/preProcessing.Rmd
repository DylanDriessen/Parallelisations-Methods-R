---
title: "PreProcessing"
output: html_notebook
---

Alle functies binnen de file preProcessing.r maken gebruik van het package [stringi](https://cran.r-project.org/web/packages/stringi/stringi.pdf) om transliteratie op een vector van strings toe te passen.

## Sequentiel

Elke lijn van de vector wordt achter elkaar doorgegeven aan de functie stringi::stri_trans_general.

```{r}
preProcess <- function(text){
  return(stringi::stri_trans_general(text, 'Latin-ASCII'))
}
```



## Parallel

Aangezien lijnen onafhankelijk van elkaar geprocessed kunnen worden (emberassingly parallel), kunnen we hier gemakkelijk parallel processing op toepassen met behulp van de packages [parallel](https://www.rdocumentation.org/packages/parallel/versions/3.5.2/topics/clusterApply) en [doparallel](https://cran.r-project.org/web/packages/doParallel/doParallel.pdf).



### clusterApply
We hebben eerst geprobeerd om lijn per lijn te verdelen over de cores. Dit bleek behoorlijk inefficient te zijn aangezien er een grote overhead bestond die te wijten was aan communicatie tussen master en slaves. 

```{r}
preProcessCluster <- function() {
  cluster <- makeCluster(no_cores, outfile = "" )
  result <- clusterApply(cl = cluster,x=docs$text,preProcessChunk)
  stopCluster(cluster)
  return(unlist(result))
}
```

#![ClusterApplyPlot](./resources/ClusterApply.png)

[
**Fig. 1:** *Uitvoeren van commando's en communicatie tussen master en slaves wanneer de data niet opgedeeld is.*
]

***

### clusterApplyChunked
Een oplossing voor deze overhead is om eerst de vector in een aantal chunks te verdelen, gelijk aan het aantal cores dat we ter beschikking hebben. Eerst deden we dit door bij elke core de volledige vector in te laden, waarna we de ID's van de lijnen verdelen over de verschillende cores. Op deze manier werd onze code sneller uitgevoerd maar was het geheugen gebruik niet optimaal. Zo werd onze originele vector, wanneer we van 8 cores gebruik maken, 9 keer in het ram geheugen ingeladen.

Oms ons geheugen verbruik te verbeteren zijn we overgeschakeld naar het opsplitsen van de text vector in chunks en deze dan te verdelen over de cores. Op deze manier wordt de vector maar 3 keer in geheugen gezet, 1 keer in de originele dataframe, 1 keer in de lijst van chunks en elke core laad dan zijn eigen chunk ook nog eens in het geheugen.

####Creating chunks

```{r}
createChunksObjects <- function(noChunks){
  chunkList <- list()
  for(i in 1:noChunks){
    #bereken de onder- en bovengrens van de chunk
    og <- round((i-1)*nrow(docs)/noChunks)+1
    bg <- round(nrow(docs)/noChunks*i)
    #Neem een subset en plaats deze in een lijst
    chunkList[[i]] <- unlist(docs[og:bg, "text"])
  }
  
  return(chunkList)
}
```

###Processing chunks
```{r}
preProcessClusterChunked <- function() {
  cluster <- makeCluster(no_cores, outfile="")
  chunks <- createChunksObjects(no_cores)
  result <- unlist(clusterApply(cluster, chunks, preProcessChunk))
  stopCluster(cluster)
  return(result)
}
```

#![ClusterApplyChunkedPlot](./resources/ClusterApplyChunked.png)
[
**Fig. 2:** *Uitvoeren van commando's en communicatie tussen master en slaves wanneer de data wel opgedeeld is.*
]

***

##Microbenchmark

```{r echo=FALSE}
print("50.000 lijnen")
readRDS("./resources/preProcessBenchmark.rds")
```

##Bevindingen
* Het parallel uitvoeren van named functions gebruikt minder ram dan wanneer we ze anonymous maken.
* Wanneer we parLappy gaan gebruiken zonder de vector eerst te verdelen in chunks, zien we dat er geen overhead is op vlak van communicatie. Toch is de functie waar we de vector eerst gaan verdelen in chunks een beetje sneller.

#![parLapplyPlot](./resources/parLapply.png)
[
**Fig. 3:** *Uivoeren van commando's en communicatie tussen master en slaves bij parlapply zonder te verdelen in chunks*
]

***